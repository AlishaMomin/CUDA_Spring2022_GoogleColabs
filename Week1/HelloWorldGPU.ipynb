{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HelloWorldGPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZUKWKjP49D7VhQqYcw83F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmmovania/CUDA_Spring2022_GoogleColabs/blob/main/Week1/HelloWorldGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBUHmmqQHOS-",
        "outputId": "98c41f6a-6306-4d60-e6e0-e65fa710670e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local\n",
            "  File: cuda -> /usr/local/cuda-10.1\n",
            "  Size: 20        \tBlocks: 0          IO Block: 4096   symbolic link\n",
            "Device: 24h/36d\tInode: 143         Links: 1\n",
            "Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)\n",
            "Access: 2022-01-10 04:16:12.192025740 +0000\n",
            "Modify: 2022-01-10 04:16:12.082025872 +0000\n",
            "Change: 2022-01-10 04:16:12.082025872 +0000\n",
            " Birth: -\n",
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-gfmnm70x\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-gfmnm70x\n",
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ]
        }
      ],
      "source": [
        "%cd /usr/local/\n",
        "!rm -rf cuda\n",
        "!ln -s /usr/local/cuda-10.1 /usr/local/cuda\n",
        "!stat cuda\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aikKbs6TMEp6",
        "outputId": "bf697f9c-0bad-42f5-ee96-f9c62c1a0dc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 10 04:16:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void HelloKernel() {\n",
        "    printf(\"\\tHello from GPU (device)\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  printf(\"Hello from CPU (host) before kernel execution\\n\");\n",
        "  HelloKernel<<<1,32>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  printf(\"Hello from CPU (host) after kernel execution\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P71PB1boHhgz",
        "outputId": "e721ca1e-d68d-48c1-98a8-e811fd97a931"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from CPU (host) before kernel execution\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "Hello from CPU (host) after kernel execution\n",
            "\n"
          ]
        }
      ]
    }
  ]
}